

<!doctype html>
<html>

<head>


<title>Dong (Carlo) An</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Dong An, 安东, CRIPAC, NLPR, CASIA, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, PKU"> 
<meta name="description" content="Dong An's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Dong (Carlo) An <h1>
				</div>
		<p>
      Center for Research on Intelligent Perception and Computing (CRIPAC) </br>
      National Laboratory of Pattern Recognition (NLPR) </br>
			Institute of Automation, Chinese Academic of Sciences (CASIA) </br>
      </br>
      Email: andong2019 AT ia.ac.cn </br>
		</p>
		<p>
			<a href="https://github.com/MarSaKi"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=8Zdbbz0AAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
			<!-- <a href="https://cn.linkedin.com/in/dong-an-9b8895159?trk=people-guest_people_search-card"><img src="assets/logos/linkedin_logo.png" height="30px"></a>&nbsp;&nbsp; -->
      <a href="https://twitter.com/andongverse"><img src="assets/logos/twitter_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/ad.jpg" width="80%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>News</h2>
<ul>
  <li>10 / 2022: &nbsp; One paper is accepted by <b>BMVC 2022!</b></li>
  <li>08 / 2022: &nbsp; We won <b>2nd place</b> in CSIG 2022 <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE Challenge</a>!</li>
  <li>07 / 2022: &nbsp; We won <b>2nd place</b> in IJCAI-ECAI 2022 <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Noisy Labels Challenge</a>! Congratulations to WeiChen!</li>
  <li>06 / 2022: &nbsp; We won <b>1st place</b> in CVPR 2022 <a href="https://embodied-ai.org/">Embodied AI Workshop</a>: <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat Competition</a>!</li>
  <li>09 / 2021: &nbsp; One paper is accepted by <b>NeurIPS 2021!</b></li>
  <li>08 / 2021: &nbsp; I was selected into the first PhD experimental class of CASIA!</li>
  <li>07 / 2021: &nbsp; One paper is accepted by <b>ACM MM2021!</b></li>
</ul>

<h2>Biography</h2> 
<p>
  Dong received the BSc degree from Peking University (PKU) in 2019. He is a PhD candidate of Institute of Automation, Chinese Academy of Sciences (CASIA), 
  supervised by <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&hl=zh-CN">Prof. Tieniu Tan</a>, 
  <a href="https://scholar.google.com/citations?hl=zh-CN&user=8kzzUboAAAAJ">Prof. Liang Wang</a> and <a href="https://yanrockhuang.github.io/">Prof. Yan Huang</a>.
  He interned at SenseTime and Huawei 2012 Lab.
  </p>
<p>
  His research interests include computer vision and pattern recognition, with a focus on Embodied-AI and Multimodal Leaning.
</p>

<!-- <h2> Selected Journal Papers</h2> 
<ul>
</ul> -->
  
  
<!-- <h2>Selected Conference Papers</h2> 
<ul>
</ul> -->


<h2>Publications</h2> 
<ul>
  <li>
    <p><strong>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</strong></p>
    <p><strong>Dong An</strong>*, Zun Wang*, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, Jing Shao</p>
    <p><i>Technical Report, 2022. <a href="https://arxiv.org/pdf/2206.11610.pdf" target="_self">PDF</a></p>
  </li>

  <li>
    <p><strong>Neighbor-view Enhanced Model for Vision and Language Navigation</strong></p>
    <p><strong>Dong An</strong>, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, Tieniu Tan</p>
    <p><i>ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. (<font color="#FF0000">Oral, 9.2% acceptance rate</font>) <a href="http://arxiv.org/abs/2107.07201" target="_self">PDF</a></p>
  </li>

  <li>
    <p><strong>Neighbor Regularized Bayesian Optimization for Hyperparameter Optimization </strong></p>
    <p>Lei Cui, Yangguang Li, Xin Lu, <strong>Dong An</strong>, Fenggang Liu</p>
    <p><i>British Machine Vision Conference (<strong>BMVC</strong>), 2022. <a href="https://arxiv.org/abs/2210.03481" target="_self">PDF</a></p>
  </li>

  <li>
    <p><strong>Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision</strong></p>
    <p>Keji He, Yan Huang, Qi Wu, Jianhua Yang, <strong>Dong An</strong>, Shuanglin Sima, Liang Wang</p>
    <p><i><i>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021. <a href="https://proceedings.neurips.cc/paper/2021/file/0602940f23884f782058efac46f64b0f-Paper.pdf" target="_self">PDF</a></p>
  </li>

</ul>


<h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/rxrhabitat_challenge.jpg" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>RxR-Habitat Vision-and-Language Navigation Challenge @ CVPR 2022 </b>.  
      Our team Joyboy (<b>Dong An</b>*, Zun Wang*, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, Jing Shao) is the <font color="#FF0000">winner</font>. See details here: <a href="https://ai.google.com/research/rxr/habitat">Results of RxR-Habitat 2022</a>.
    </p>
  </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/reverie_challenge.jpg" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>REVERIE Challenge @ CSIG 2022</b>. 
      Our team TouchFish (<b>Dong An</b>, Yifei Su, Shuanglin Sima, Hongyuan Yu, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of both channels. 
      See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>Learning and Mining with Noisy Labels Challenge @ IJCAI-ECAI 2022</b>.  
      Our team (Weichen Yu, Hongyuan Yu, Yan Huang, <b>Dong An</b>, Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. 
      See details here: <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
  <li><p>Winner invited talk at Embodied-AI Workshop @ CVPR 2022 </p></li>
  <li><p>Reviewer, ACM MM </p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
  <li><p>2022, <font color="#FF0000">Winner</font> of RxR-Habitat Challenge, CVPR 2022 [<a href="assets\cert\rxr-habitat-cert.pdf" target="blank">RxR-Habitat Certificate</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of REVERIE Challenge, CSIG 2022 [<a href="assets\cert\赛道1 touchfish.pdf" target="blank">REVERIE Certificate_1</a>] [<a href="assets\cert\赛道2 touchfish.pdf" target="blank">REVERIE Certificate_2</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of Learning and Mining with Noisy Labels Challenge, IJCAI-ECAI 2022 [<a href="assets\cert\noisy-label-cert.pdf" target="blank">Noisy-Labels Certificate</a>]</p> </li>
  <li><p>2021, Outstanding Student of CAS</p> </li>
</ul>


<table width="100%"> 
	<tr> 
		<td align="center">&copy; Dong An | Last update: Oct 2022</td>
	</tr> 
</table>

</div>


</body>

</html>

