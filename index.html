

<!doctype html>
<html>

<head>


<title>Dong (Carlo) An</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Dong An, 安东, MBZUAI"> 
<meta name="description" content="Dong An's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Dong (Carlo) An <h1>
				</div>
		<p>
      Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) </br>
      </br>
      Email: dong[dot]an[at]mbzuai[dot]ac[dot]ae </br>
		</p>
		<p>
			<a href="https://github.com/MarSaKi"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=8Zdbbz0AAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
      <a href="https://twitter.com/andongverse"><img src="assets/logos/twitter_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/ad.jpeg" width="66%"/>
			</td>
		<tr>
	</tbody>
</table>

<!-- <h2>News</h2>
<ul>
  <li>08 / 2023: &nbsp; We won <b>1st place</b> in ICCV 2023 <a href="https://iccv-clvl.github.io/2023/">CLVL Workshop</a>: <a href="https://sites.google.com/view/aerial-vision-and-dialog/avdn-challenge?authuser=0">AVDN Competition</a>! Congratulations to Yifei!</li>
  <li>07 / 2023: &nbsp; One paper is accepted by <b>ICCV 2023!</b></li>
  <li>10 / 2022: &nbsp; One paper is accepted by <b>BMVC 2022!</b></li>
  <li>08 / 2022: &nbsp; We won <b>2nd place</b> in CSIG 2022 <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE Challenge</a>!</li>
  <li>07 / 2022: &nbsp; We won <b>2nd place</b> in IJCAI-ECAI 2022 <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Noisy Labels Challenge</a>! Congratulations to Weichen!</li>
  <li>06 / 2022: &nbsp; We won <b>1st place</b> in CVPR 2022 <a href="https://embodied-ai.org/">Embodied AI Workshop</a>: <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat Competition</a>!</li>
  <li>09 / 2021: &nbsp; One paper is accepted by <b>NeurIPS 2021!</b></li>
  <li>07 / 2021: &nbsp; One paper is accepted by <b>ACM MM2021!</b></li>
</ul> -->

<h2>About Me</h2> 
<p>
  I'm currently a Postdoctoral Researcher at <a href="https://www.mbzuai.ac.ae/">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</a>, advised by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.
  Prior to that, I received the PhD degree from Institute of Automation, Chinese Academy of Sciences (CASIA) in 2024, advised by <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&hl=zh-CN">Prof. Tieniu Tan</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=8kzzUboAAAAJ">Prof. Liang Wang</a> and <a href="https://yanrockhuang.github.io/">Prof. Yan Huang</a>.
  I got the bachelor degree from Peking University (PKU) in 2019.
  My research interests lie in the intersection of deep learning and robotics, with a focus on Embodied AI and Multimodal Learning.
</p>

<h2>Publications</h2> 
<ul>
  <li>
    <p><strong>ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments</strong></p>
    <p><strong>Dong An</strong>, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, Liang Wang</p>
    <p>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024. 
      <a href="https://ieeexplore.ieee.org/document/10495141" target="_self">[Paper]</a>
      <a href="https://github.com/MarSaKi/ETPNav" target="_self">[Code]</a></p>
  </li>

  <li>
    <p><strong>BEVBert: Multimodal Map Pre-training for Language-guided Navigation</strong></p>
    <p><strong>Dong An</strong>, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao</p>
    <p>International Conference on Computer Vision (<strong>ICCV</strong>), 2023. 
      <a href="https://arxiv.org/abs/2212.04385" target="_self">[Paper]</a>
      <a href="https://github.com/MarSaKi/VLN-BEVBert" target="_self">[Code]</a></p>
  </li>
  
  <li>
    <p><strong>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition</strong></p>
    <p><strong>Dong An</strong>*, Zun Wang*, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, Jing Shao</p>
    <p>Room-Across-Room (RxR) Habitat Challenge (<strong>CVPR Embodied AI Workshop</strong>), 2022. 
      <a href="https://arxiv.org/abs/2206.11610" target="_self">[Paper]</a></p>
  </li>

  <li>
    <p><strong>Neighbor-view Enhanced Model for Vision and Language Navigation</strong></p>
    <p><strong>Dong An</strong>, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, Tieniu Tan</p>
    <p>ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. (<font color="#FF0000">Oral</font>) 
      <a href="http://arxiv.org/abs/2107.07201" target="_self">[Paper]</a>
      <a href="https://github.com/MarSaKi/NvEM" target="_self">[Code]</a></p>
  </li>

  <li>
    <p><strong>Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments</strong></p>
    <p>Kehan Chen*, <strong>Dong An</strong>*, Yan Huang, Rongtao Xu, Yifei Su, Yonggen Ling, Ian Reid, Liang Wang</p>
    <p>Preprint, 2025. 
      <a href="https://arxiv.org/pdf/2412.10137" target="_self">[Paper]</a>
      <a href="https://chenkehan21.github.io/CA-Nav-project/" target="_self">[Webpage]</a>
      <a href="https://github.com/Chenkehan21/CA-Nav-code" target="_self">[Code]</a></p>
  </li>

  <li>
    <p><strong>Learning Fine-Grained Alignment for Aerial Vision-Dialog Navigation</strong></p>
    <p>Yifei Su, <strong>Dong An</strong>, Kehan Chen, Weichen Yu, Baiyang Ning, Yonggen Ling, Yan Huang, Liang Wang</p>
    <p>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025.</p>
  </li>

  <li>
    <p><strong>Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models</strong></p>
    <p>Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, <strong>Dong An</strong>, Ian Reid</p>
    <p>Workshop on Planning in the Era of LLMs (<strong>LM4Plan @ AAAI</strong>), 2025.
        <a href="https://arxiv.org/pdf/2501.05478" target="_self">[Paper]</a></p>
  </li>

  <li>
    <p><strong>Memory-Adaptive Vision-and-Language Navigation</strong></p>
    <p>Keji He, Ya Jing, Yan Huang, Zhihe Lu, <strong>Dong An</strong>, Liang Wang</p>
    <p>Pattern Recognition (<strong>PR</strong>), 2024. 
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324002620" target="_self">[Paper]</a></p>
  </li>

  <li>
    <p><strong>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation</strong></p>
    <p>Yifei Su, <strong>Dong An</strong>, Yuan Xu, Kehan Chen, Yan Huang</p>
    <p>Aerial Vision-and-Dialog Navigation (AVDN) Challenge (<strong>ICCV CLVL Workshop</strong>), 2023. 
      <a href="https://arxiv.org/pdf/2308.11561" target="_self">[Paper]</a>
      <a href="https://github.com/yifeisu/TG-GAT" target="_self">[Code]</a></p>
  </li>

  <li>
    <p><strong>Neighbor Regularized Bayesian Optimization for Hyperparameter Optimization </strong></p>
    <p>Lei Cui, Yangguang Li, Xin Lu, <strong>Dong An</strong>, Fenggang Liu</p>
    <p>British Machine Vision Conference (<strong>BMVC</strong>), 2022. 
      <a href="https://arxiv.org/abs/2210.03481" target="_self">[Paper]</a></p>
  </li>

  <li>
    <p><strong>Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision</strong></p>
    <p>Keji He, Yan Huang, Qi Wu, Jianhua Yang, <strong>Dong An</strong>, Shuanglin Sima, Liang Wang</p>
    <p>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021. 
      <a href="https://proceedings.neurips.cc/paper/2021/hash/0602940f23884f782058efac46f64b0f-Abstract.html" target="_self">[Paper]</a>
      <a href="https://github.com/hekj/Landmark-RxR" target="_self">[Code]</a></p>
  </li>

</ul>


<h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/avdn_challenge.png" alt="alt text" width="66" height="66" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>Aerial Vision-and-Dialogue Navigation Challenge @ ICCV 2023 </b>.  
      Our team susanping (Yifei Su, <b>Dong An</b>, Yuan Xu, Kehan Chen, Yan Huang) is the <font color="#FF0000">winner</font>. See details here: <a href="https://sites.google.com/view/aerial-vision-and-dialog/avdn-challenge?authuser=0">Results of AVDN Challenge 2023</a>.
    </p>
  </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/rxrhabitat_challenge.jpg" alt="alt text" width="66" height="66" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>RxR-Habitat Vision-and-Language Navigation Challenge @ CVPR 2022 </b>.  
      Our team Joyboy (<b>Dong An</b>*, Zun Wang*, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang, Jing Shao) is the <font color="#FF0000">winner</font>. See details here: <a href="https://ai.google.com/research/rxr/habitat">Results of RxR-Habitat 2022</a>.
    </p>
  </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/reverie_challenge.jpg" alt="alt text" width="66" height="66" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>REVERIE Challenge @ CSIG 2022</b>. 
      Our team TouchFish (<b>Dong An</b>, Yifei Su, Shuanglin Sima, Hongyuan Yu, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of both channels. 
      See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="66" height="66" /> &nbsp;</td>
    <td align="left">
    <p> 
      <b>Learning and Mining with Noisy Labels Challenge @ IJCAI-ECAI 2022</b>.  
      Our team (Weichen Yu, Hongyuan Yu, Yan Huang, <b>Dong An</b>, Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. 
      See details here: <a href="http://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
  <li><p>Winner invited talk at Embodied AI Workshop @ CVPR 2022 </p></li>
  <li><p>Reviewer: ACM MM'21, IJCAI'23, NeurIPS'23, AAAI'24, CVPR'24, ECCV'24, ACM MM'24, NeurIPS'24, ICLR'25, ICML'25 </p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
  <li><p>2023, <font color="#FF0000">Winner</font> of AVDN Challenge, ICCV 2023 [<a href="assets\cert\avdn-cert.pdf" target="blank">AVDN Certificate</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Winner</font> of RxR-Habitat Challenge, CVPR 2022 [<a href="assets\cert\rxr-habitat-cert.pdf" target="blank">RxR-Habitat Certificate</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of REVERIE Challenge, CSIG 2022 [<a href="assets\cert\赛道1 touchfish.pdf" target="blank">REVERIE Certificate_1</a>] [<a href="assets\cert\赛道2 touchfish.pdf" target="blank">REVERIE Certificate_2</a>]</p> </li>
  <li><p>2022, <font color="#FF0000">Runner-up</font> of Learning and Mining with Noisy Labels Challenge, IJCAI-ECAI 2022 [<a href="assets\cert\noisy-label-cert.pdf" target="blank">Noisy-Labels Certificate</a>]</p> </li>
  <li><p>2021, Outstanding Student of CAS</p> </li>
</ul>

<h2> </h2> 
<table width="100%"> 
	<tr> 
    <td align="center"> 
      Any sufficiently advanced technology is indistinguishable from magic. -- Arthur C. Clarke
    </td>
	</tr> 
</table>

<table width="100%"> 
	<tr> 
    <td align="center"> 
      Next token prediction is actually a much deeper question than it seems. -- Ilya Sutskever
    </td>
	</tr> 
</table>

</div>

</body>

</html>